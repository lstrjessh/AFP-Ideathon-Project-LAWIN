{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FGSC-23 Baseline (ResNet50, PyTorch)\n",
        "\n",
        "This notebook trains a baseline fine-grained ship classifier on FGSC-23 using transfer learning.\n",
        "\n",
        "- Dataset: `AFP Ideathon/FGSC-23/`\n",
        "- Classes: 23 (see mapping in `readme.txt`)\n",
        "- Model: ResNet50 (ImageNet pretrained)\n",
        "- Metrics: accuracy, confusion matrix\n",
        "- Output: best model checkpoint and simple inference demo\n",
        "\n",
        "If running on Windows without CUDA, training will be slower; consider reducing batch size/epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment checks and installs (run once if needed)\n",
        "import sys, subprocess, pkgutil\n",
        "\n",
        "required = [\n",
        "    'torch', 'torchvision', 'tqdm', 'matplotlib', 'scikit-learn'\n",
        "]\n",
        "for pkg in required:\n",
        "    if pkg not in {m.name for m in pkgutil.iter_modules()}:\n",
        "        print(f'Installing {pkg}...')\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "print('Python:', sys.version)\n",
        "print('PyTorch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "assert torch.cuda.is_available(), 'CUDA not available. Please install CUDA-enabled PyTorch or select a GPU runtime.'\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = r'C:\\\\Users\\\\LESTER\\\\Desktop\\\\Codes'\n",
        "DATA_ROOT = os.path.join(PROJECT_ROOT, 'AFP Ideathon', 'FGSC-23')\n",
        "TRAIN_DIR = os.path.join(DATA_ROOT, 'train')\n",
        "TEST_DIR = os.path.join(DATA_ROOT, 'test')\n",
        "NUM_CLASSES = 23\n",
        "assert os.path.isdir(TRAIN_DIR) and os.path.isdir(TEST_DIR), 'FGSC-23 train/test folders not found'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transforms and Datasets\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 6  # increase if you have more CPU cores\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(TEST_DIR, transform=test_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "print('Classes:', class_names)\n",
        "print('Train images:', len(train_ds), ' Val images:', len(val_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model: ResNet50 transfer learning\n",
        "from torchvision.models import resnet50\n",
        "import torch.nn as nn\n",
        "\n",
        "model = resnet50(weights='IMAGENET1K_V1')\n",
        "# Freeze backbone (optional); unfreeze last layer\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "print('Device:', DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training utilities\n",
        "import torch.optim as optim\n",
        "\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_path = os.path.join(DATA_ROOT, 'resnet50_fgsc23_best.pt')\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(loader, desc='Train', leave=False):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc='Val', leave=False):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().tolist())\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "    return running_loss / total, correct / total, all_preds, all_labels\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
        "    print(f'Epoch {epoch:02d}/{epochs} | train_loss={train_loss:.4f} acc={train_acc:.4f} | val_loss={val_loss:.4f} acc={val_acc:.4f}')\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f'  Saved new best to {best_path} (val_acc={best_val_acc:.4f})')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model and evaluate on the test set with confusion matrix\n",
        "import numpy as np\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "best = resnet50(weights=None)\n",
        "in_features = best.fc.in_features\n",
        "best.fc = nn.Linear(in_features, NUM_CLASSES)\n",
        "best = best.to(DEVICE)\n",
        "best.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
        "\n",
        "_, test_acc, test_preds, test_labels = evaluate(best, val_loader, criterion, DEVICE)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ConfusionMatrixDisplay(cm, display_labels=class_names).plot(ax=ax, cmap='Blues', colorbar=False, xticks_rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nClassification report:\\n')\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick inference demo on a few test images\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "best.eval()\n",
        "sample_paths = []\n",
        "for cls in class_names[:5]:  # sample from first 5 classes for brevity\n",
        "    cls_dir = os.path.join(TEST_DIR, cls)\n",
        "    imgs = [f for f in os.listdir(cls_dir) if f.lower().endswith('.jpg')]\n",
        "    if imgs:\n",
        "        sample_paths.append(os.path.join(cls_dir, random.choice(imgs)))\n",
        "\n",
        "for p in sample_paths:\n",
        "    img = Image.open(p).convert('RGB')\n",
        "    inp = test_tfms(img).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits = best(inp)\n",
        "        pred = logits.argmax(dim=1).item()\n",
        "    print(f'Image: {os.path.basename(p)} | Pred: {class_names[pred]}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
